{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-28 00:39:28 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230228-003928.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-28 00:39:29 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pyarrow.parquet as pq\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = './data/train'\n",
    "meta_data_path = './data/train_meta.parquet'\n",
    "geometry_table = pd.read_csv('./data/sensor_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        batch_id = meta_batch['batch_id'].unique()\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      engine: sqlalchemy.engine.base.Engine) -> None:\n",
    "                      \n",
    "    try:\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "   \n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000,\n",
    "                      batch_ids: list = [],\n",
    "                      engine: sqlalchemy.engine.base.Engine = None\n",
    "                      ) -> None:\n",
    "    \n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    batch_id = 1\n",
    "    converted_batches = []\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        if batch_id in batch_ids:\n",
    "            meta_data_batch  = meta_data_batch.to_pandas()\n",
    "            add_to_table(database_path = database_path,\n",
    "                        df = meta_data_batch,\n",
    "                        table_name='meta_table',\n",
    "                        is_primary_key= True,\n",
    "                        engine = engine)\n",
    "            pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "            del meta_data_batch \n",
    "            add_to_table(database_path = database_path,\n",
    "                        df = pulses,\n",
    "                        table_name='pulse_table',\n",
    "                        is_primary_key= False,\n",
    "                        engine = engine)\n",
    "            del pulses \n",
    "            converted_batches.append(batch_id)\n",
    "        batch_id +=1\n",
    "        if len(batch_ids) == len(converted_batches):\n",
    "            break\n",
    "        gc.collect()\n",
    "    del meta_data_iter \n",
    "    print(f'Conversion Complete! Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_dict = {}\n",
    "# list_train_ids = range(1,661)\n",
    "# for batch_number in range(0,5):\n",
    "#     list_dict[batch_number] = np.random.choice(list_train_ids, 110, replace=False)\n",
    "#     list_train_ids = [x for x in list_train_ids if x not in list_dict[batch_number]]\n",
    "#     print(f'Batch {batch_number} contains {len(list_dict[batch_number])} events')\n",
    "\n",
    "\n",
    "# with open('5_big_batch_indx.pkl', 'wb') as f:\n",
    "#     pickle.dump(list_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = pd.read_pickle('5_big_batch_indx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [3:44:05<00:00, 13445.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Complete! Database available at\n",
      " ./data/extra_big_batch_0.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_number in tqdm([0]):\n",
    "    database_path = f'./data/extra_big_batch_{batch_number}.db'\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    convert_to_sqlite(meta_data_path,\n",
    "                    database_path=database_path,\n",
    "                    input_data_folder=input_data_folder,\n",
    "                    batch_size=200000,\n",
    "                    batch_ids=list_dict[batch_number],\n",
    "                    engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f831811ab5408f3cdb83b41500deeb387c9454d72d6267bf8a6ce625eb23eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
