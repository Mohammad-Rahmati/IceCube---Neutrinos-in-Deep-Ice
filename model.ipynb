{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-16 08:50:55 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230216-085055.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-16 08:50:56 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Any, Dict, List\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.adam import Adam\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa\n",
    "from graphnet.training.callbacks import PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == 'direction':\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "        additional_attributes = ['zenith', 'azimuth', 'event_id']\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=Adam,\n",
    "        optimizer_kwargs={\"lr\": 1e-03, \"eps\": 1e-03},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-02, 1, 1e-02],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    train_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist(),\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = True,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    validate_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist(),\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                          \n",
    "                                            )\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "def train_dynedge_from_scratch(config: Dict[str, Any]) -> StandardModel:\n",
    "    \"\"\"Builds and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "    \n",
    "    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n",
    "    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(config = config)\n",
    "\n",
    "    model = build_model(config, train_dataloader)\n",
    "\n",
    "    # Training model\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        **config[\"fit\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n",
    "    # Make Dataloader\n",
    "    test_dataloader = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = None, # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=model.additional_attributes,\n",
    "    )\n",
    "    # Save predictions and model to file\n",
    "    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n",
    "    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n",
    "    db_name = config['path'].split(\"/\")[-1].split(\".\")[0]\n",
    "    path = os.path.join(archive, db_name, run_name)\n",
    "    logger.info(f\"Writing results to {path}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    results.to_csv(f\"{path}/results.csv\")\n",
    "    return results\n",
    "\n",
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df['true_x'] = np.cos(df['azimuth']) * np.sin(df['zenith'])\n",
    "    df['true_y'] = np.sin(df['azimuth'])*np.sin(df['zenith'])\n",
    "    df['true_z'] = np.cos(df['zenith'])\n",
    "    return df\n",
    "def calculate_angular_error(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df['angular_error'] = np.arccos(df['true_x']*df['direction_x'] + df['true_y']*df['direction_y'] + df['true_z']*df['direction_z'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "features = FEATURES.KAGGLE\n",
    "truth = TRUTH.KAGGLE\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "        \"path\": './data/database/database_51-55.db',\n",
    "        \"inference_database_path\": './data/database/database_1.db',\n",
    "        \"pulsemap\": 'pulse_table',\n",
    "        \"truth_table\": 'meta_table',\n",
    "        \"features\": features,\n",
    "        \"truth\": truth,\n",
    "        \"index_column\": 'event_id',\n",
    "        \"run_name_tag\": 'my_example',\n",
    "        \"batch_size\": 100,\n",
    "        \"num_workers\": 16,\n",
    "        \"target\": 'direction',\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"fit\": {\n",
    "                \"max_epochs\": 50,\n",
    "                \"gpus\": [0],\n",
    "                \"distribution_strategy\": None,\n",
    "                },\n",
    "        'train_selection': './data/train_selection_max_200_pulses.csv',\n",
    "        'validate_selection': './data/validate_selection_max_200_pulses.csv',\n",
    "        'test_selection': None,\n",
    "        'base_dir': 'training'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_dynedge_from_scratch(config = config)\n",
    "results = inference(model, config)\n",
    "\n",
    "results = convert_to_3d(results)\n",
    "results = calculate_angular_error(results)\n",
    "results.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f831811ab5408f3cdb83b41500deeb387c9454d72d6267bf8a6ce625eb23eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
