{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-25 23:02:14 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230225-230214.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-25 23:02:15 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(database_path)\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        accepted_batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    \n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "        \n",
    "    converted_batches = [] \n",
    "    progress_bar = tqdm(total = None)\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        unique_batch_ids = pd.unique(meta_data_batch['event_id']).tolist()\n",
    "        meta_data_batch  = meta_data_batch.to_pandas()\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = meta_data_batch,\n",
    "                    table_name='meta_table',\n",
    "                    is_primary_key= True)\n",
    "        pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "        del meta_data_batch # memory\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = pulses,\n",
    "                    table_name='pulse_table',\n",
    "                    is_primary_key= False)\n",
    "        del pulses # memory\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/test_database.db\n",
      "./data/test_database.db\n",
      "Conversion Complete!. Database available at\n",
      " ./data/test_database.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_data_folder = './data/test/'\n",
    "geometry_table = pd.read_csv('./data/sensor_geometry.csv')\n",
    "meta_data_path = './data/test_meta.parquet'\n",
    "\n",
    "database_path = './data/test_database.db'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.adam import Adam\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, ZenithReconstructionWithKappa, AzimuthReconstructionWithKappa\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n",
    "from graphnet.utilities.logging import get_logger\n",
    "from pytorch_lightning import Trainer\n",
    "import pandas as pd\n",
    "\n",
    "def build_model(config: Dict[str,Any], train_dataloader: Any, pooling_list: List) -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=pooling_list,\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == 'direction':\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "        additional_attributes = ['zenith', 'azimuth', 'event_id']\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=Adam,\n",
    "        optimizer_kwargs={\"lr\": 1e-03, \"eps\": 1e-03},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-02, 1, 1e-02],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    \n",
    "    train_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist() if config['train_selection'] else None,\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = True,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    validate_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist() if config['validate_selection'] else None,\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                          \n",
    "                                            )\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "def load_pretrained_model(config: Dict[str,Any], state_dict_path: str = '', pooling_list: List = []) -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config = config)\n",
    "    model = build_model(config = config, \n",
    "                        train_dataloader = train_dataloader,\n",
    "                        pooling_list=pooling_list)\n",
    "    \n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "    model.additional_attributes = ['event_id']\n",
    "    return model\n",
    "\n",
    "\n",
    "def inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n",
    "    # Make Dataloader\n",
    "    test_dataloader = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = None, # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = None, # Cannot make labels in test data\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=['event_id']\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df, angle_post_fix = '_reco', vec_post_fix = '') -> pd.DataFrame:\n",
    "    r = np.sqrt(df['direction_x'+ vec_post_fix]**2 + df['direction_y'+ vec_post_fix]**2 + df['direction_z' + vec_post_fix]**2)\n",
    "    df['zenith' + angle_post_fix] = np.arccos(df['direction_z'+ vec_post_fix]/r)\n",
    "    df['azimuth'+ angle_post_fix] = np.arctan2(df['direction_y'+ vec_post_fix],df['direction_x' + vec_post_fix]) #np.sign(results['true_y'])*np.arccos((results['true_x'])/(np.sqrt(results['true_x']**2 + results['true_y']**2)))\n",
    "    df['azimuth'+ angle_post_fix][df['azimuth'  + angle_post_fix]<0] = df['azimuth'  + angle_post_fix][df['azimuth'  +  angle_post_fix]<0] + 2*np.pi \n",
    "\n",
    "    return df[['event_id', 'azimuth', 'zenith']].set_index('event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "features = FEATURES.KAGGLE\n",
    "truth = TRUTH.KAGGLE\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "        \"path\": './data/test_database.db',\n",
    "        \"inference_database_path\": './data/test_database.db',\n",
    "        \"pulsemap\": 'pulse_table',\n",
    "        \"truth_table\": 'meta_table',\n",
    "        \"features\": features,\n",
    "        \"truth\": truth,\n",
    "        \"index_column\": 'event_id',\n",
    "        \"run_name_tag\": 'submission',\n",
    "        \"batch_size\": 1,\n",
    "        \"num_workers\": 32,\n",
    "        \"target\": 'direction',\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"fit\": {\n",
    "                \"max_epochs\": 50,\n",
    "                \"gpus\": [0],\n",
    "                \"distribution_strategy\": None,\n",
    "                },\n",
    "        'train_selection': None,\n",
    "        'validate_selection':  None,\n",
    "        'test_selection': None,\n",
    "        'base_dir': 'training'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = load_pretrained_model(config = config, state_dict_path = 'm4.pth', pooling_list=[\"min\", \"max\", \"mean\", \"sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader_1 = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = None, # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = None, # Cannot make labels in test data\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "\n",
    "test_dataloader_2 = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = [2092], # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = None, # Cannot make labels in test data\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "\n",
    "len(test_dataloader_1), len(test_dataloader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def run_parallel(func1, func2):\n",
    "    # Create two processes to execute each function\n",
    "    process1 = multiprocessing.Process(target=func1)\n",
    "    process2 = multiprocessing.Process(target=func2)\n",
    "    \n",
    "    # Start both processes\n",
    "    process1.start()\n",
    "    process2.start()\n",
    "    \n",
    "    # Wait for both processes to finish\n",
    "    process1.join()\n",
    "    process2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-26 00:40:24 - StandardModel.warning - A `Trainer` instance has already been constructed, possibly when the model was trained. Will use this to get predictions. Argument `gpus = [0]` will be ignored.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f3095920280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-26 00:40:24 - StandardModel.warning - A `Trainer` instance has already been constructed, possibly when the model was trained. Will use this to get predictions. Argument `gpus = [0]` will be ignored.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self._shutdown_workers()\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f3095920280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "Process Process-1159:\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    Traceback (most recent call last):\n",
      "if w.is_alive():  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/tmp/ipykernel_1812843/99361372.py\", line 1, in <lambda>\n",
      "    func1 = lambda: model.predict_as_dataframe(\n",
      "      File \"/home/mo/graphnet/src/graphnet/models/model.py\", line 181, in predict_as_dataframe\n",
      "    predictions_torch = self.predict(\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/mo/graphnet/src/graphnet/models/standard_model.py\", line 173, in predict\n",
      "    return super().predict(\n",
      "\n",
      "  File \"/home/mo/graphnet/src/graphnet/models/model.py\", line 135, in predict\n",
      "    predictions_list = self._inference_trainer.predict(self, dataloader)\n",
      "AssertionError  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 892, in predict\n",
      "    return call._call_and_handle_interrupt(\n",
      ":   File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "can only test a child process  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 938, in _predict_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1048, in _run\n",
      "    self.strategy.setup_environment()\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 131, in setup_environment\n",
      "    self.accelerator.setup_device(self.root_device)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/accelerators/cuda.py\", line 44, in setup_device\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 313, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 206, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "Process Process-1160:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1812843/99361372.py\", line 8, in <lambda>\n",
      "    func2 = lambda: model.predict_as_dataframe(\n",
      "  File \"/home/mo/graphnet/src/graphnet/models/model.py\", line 181, in predict_as_dataframe\n",
      "    predictions_torch = self.predict(\n",
      "  File \"/home/mo/graphnet/src/graphnet/models/standard_model.py\", line 173, in predict\n",
      "    return super().predict(\n",
      "  File \"/home/mo/graphnet/src/graphnet/models/model.py\", line 135, in predict\n",
      "    predictions_list = self._inference_trainer.predict(self, dataloader)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 892, in predict\n",
      "    return call._call_and_handle_interrupt(\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 938, in _predict_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1048, in _run\n",
      "    self.strategy.setup_environment()\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 131, in setup_environment\n",
      "    self.accelerator.setup_device(self.root_device)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/accelerators/cuda.py\", line 44, in setup_device\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 313, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/mo/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 206, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "func1 = lambda: model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader_1,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=['event_id']\n",
    "    )\n",
    "\n",
    "func2 = lambda: model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader_2,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=['event_id']\n",
    "    )\n",
    "\n",
    "run_parallel(func1, func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-26 00:42:15 - StandardModel.warning - A `Trainer` instance has already been constructed, possibly when the model was trained. Will use this to get predictions. Argument `gpus = [0]` will be ignored.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b426c6533764bc0a9f9feaf98f81d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>direction_x</th>\n",
       "      <th>direction_y</th>\n",
       "      <th>direction_z</th>\n",
       "      <th>direction_kappa</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.347481</td>\n",
       "      <td>0.807123</td>\n",
       "      <td>0.477293</td>\n",
       "      <td>224287280.0</td>\n",
       "      <td>2092.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   direction_x  direction_y  direction_z  direction_kappa  event_id\n",
       "0    -0.347481     0.807123     0.477293      224287280.0    2092.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f831811ab5408f3cdb83b41500deeb387c9454d72d6267bf8a6ce625eb23eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
