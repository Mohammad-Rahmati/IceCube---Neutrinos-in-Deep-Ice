{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(database_path)\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        accepted_batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    \n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "        \n",
    "    converted_batches = [] \n",
    "    progress_bar = tqdm(total = None)\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        unique_batch_ids = pd.unique(meta_data_batch['event_id']).tolist()\n",
    "        meta_data_batch  = meta_data_batch.to_pandas()\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = meta_data_batch,\n",
    "                    table_name='meta_table',\n",
    "                    is_primary_key= True)\n",
    "        pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "        del meta_data_batch # memory\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = pulses,\n",
    "                    table_name='pulse_table',\n",
    "                    is_primary_key= False)\n",
    "        del pulses # memory\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(database_path)\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        accepted_batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    \n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "        \n",
    "    converted_batches = [] \n",
    "    progress_bar = tqdm(total = None)\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        unique_batch_ids = pd.unique(meta_data_batch['event_id']).tolist()\n",
    "        meta_data_batch  = meta_data_batch.to_pandas()\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = meta_data_batch,\n",
    "                    table_name='meta_table',\n",
    "                    is_primary_key= True)\n",
    "        pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "        del meta_data_batch # memory\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = pulses,\n",
    "                    table_name='pulse_table',\n",
    "                    is_primary_key= False)\n",
    "        del pulses # memory\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
