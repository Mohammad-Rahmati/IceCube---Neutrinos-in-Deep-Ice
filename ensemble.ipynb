{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_polar(df) -> pd.DataFrame:\n",
    "    r = np.sqrt(df['direction_x']**2 + df['direction_y']**2 + df['direction_z']**2)\n",
    "    df['zenith'] = np.arccos(df['direction_z']/r)\n",
    "    df['azimuth'] = np.arctan2(df['direction_y'],df['direction_x'])\n",
    "    df['azimuth'][df['azimuth']<0] = df['azimuth'][df['azimuth']<0] + 2*np.pi \n",
    "\n",
    "    return df[['azimuth', 'zenith', 'direction_kappa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xyz(df) -> pd.DataFrame:\n",
    "    df['x'] = np.sin(df['zenith'])*np.cos(df['azimuth'])\n",
    "    df['y'] = np.sin(df['zenith'])*np.sin(df['azimuth'])\n",
    "    df['z'] = np.cos(df['zenith'])\n",
    "    return df[['x', 'y', 'z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('data/F4/focus_batch_4.db') as con:\n",
    "        query = 'select * from meta_table'\n",
    "        meta_df = pd.read_sql(query,con)\n",
    "\n",
    "meta_df = meta_df[['event_id', 'azimuth', 'zenith']].set_index('event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = convert_to_xyz(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df_copy = target_df.copy()\n",
    "target_df_copy.rename(columns={'x':'target_x', 'y':'target_y', 'z':'target_z'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = pd.read_pickle('inference/pred_M0_F4.pkl')\n",
    "p0.index = p0.index.astype(int)\n",
    "\n",
    "p1 = pd.read_pickle('inference/pred_M1_F4.pkl')\n",
    "p1.index = p1.index.astype(int)\n",
    "\n",
    "p2 = pd.read_pickle('inference/pred_M2_F4.pkl')\n",
    "p2.index = p2.index.astype(int)\n",
    "\n",
    "p3 = pd.read_pickle('inference/pred_M3_F4.pkl')\n",
    "p3.index = p3.index.astype(int)\n",
    "\n",
    "p0.rename(columns={'direction_x':'x0', 'direction_y':'y0', 'direction_z':'z0', 'direction_kappa':'k0'}, inplace=True)\n",
    "p1.rename(columns={'direction_x':'x1', 'direction_y':'y1', 'direction_z':'z1', 'direction_kappa':'k1'}, inplace=True)\n",
    "p2.rename(columns={'direction_x':'x2', 'direction_y':'y2', 'direction_z':'z2', 'direction_kappa':'k2'}, inplace=True)\n",
    "p3.rename(columns={'direction_x':'x3', 'direction_y':'y3', 'direction_z':'z3', 'direction_kappa':'k3'}, inplace=True)\n",
    "\n",
    "p0.shape, p1.shape, p2.shape, p3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([p0, p1, p2, p3, target_df_copy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.90, random_state=0)\n",
    "validation = data.drop(train.index)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "validation.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train.shape, validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Define neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train.shape[1] - 3\n",
    "batch_size = 2048\n",
    "# Prepare data\n",
    "train_data = train.iloc[:, :n_features].values\n",
    "train_targets = train.iloc[:, n_features:].values\n",
    "validation_data = validation.iloc[:, :n_features].values\n",
    "validation_targets = validation.iloc[:, n_features:].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.float32)\n",
    "validation_data = torch.tensor(validation_data, dtype=torch.float32)\n",
    "validation_targets = torch.tensor(validation_targets, dtype=torch.float32)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "validation_dataset = CustomDataset(validation_data, validation_targets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork(n_features, 3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "min_validation_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0\n",
    "        for batch, (inputs, targets) in enumerate(validation_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(validation_loader)\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        torch.save(model, \"ensemble/ensemble.pth\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Model Saved\")\n",
    "\n",
    "    else:    \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
