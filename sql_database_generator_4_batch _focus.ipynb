{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-15 10:55:02 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230315-105502.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-03-15 10:55:03 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pyarrow.parquet as pq\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = './data/train'\n",
    "meta_data_path = './data/train_meta.parquet'\n",
    "geometry_table = pd.read_csv('./data/sensor_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        batch_id = meta_batch['batch_id'].unique()\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "        \n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      engine: sqlalchemy.engine.base.Engine) -> None:\n",
    "                      \n",
    "    try:\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "   \n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      batch_size: int = 200000,\n",
    "                      batch_ids: list = [],\n",
    "                      event_ids: list = [],\n",
    "                      engine: sqlalchemy.engine.base.Engine = None\n",
    "                      ) -> None:\n",
    "    \n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    batch_id = 1\n",
    "    converted_batches = []\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        print(batch_id, end=',')\n",
    "        if batch_ids % 110 == 0:\n",
    "            print('\\n')\n",
    "        if batch_id in batch_ids:\n",
    "            meta_data_batch  = meta_data_batch.to_pandas()\n",
    "            meta_data_batch = meta_data_batch.loc[meta_data_batch['event_id'].isin(event_ids)]\n",
    "            if meta_data_batch.shape[0] > 0:\n",
    "                add_to_table(database_path = database_path,\n",
    "                            df = meta_data_batch,\n",
    "                            table_name='meta_table',\n",
    "                            is_primary_key= True,\n",
    "                            engine = engine)\n",
    "                pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n",
    "                del meta_data_batch \n",
    "                pulses = pulses.loc[pulses['event_id'].isin(event_ids)]\n",
    "                pulses = pulses.groupby('event_id', group_keys=True).apply(lambda x: x.iloc[:500])\n",
    "                if pulses.shape[0] > 0:\n",
    "                    add_to_table(database_path = database_path,\n",
    "                                df = pulses,\n",
    "                                table_name='pulse_table',\n",
    "                                is_primary_key= False,\n",
    "                                engine = engine)\n",
    "                del pulses\n",
    "            converted_batches.append(batch_id)\n",
    "        batch_id +=1\n",
    "        if len(batch_ids) == len(converted_batches):\n",
    "            break\n",
    "        gc.collect()\n",
    "    del meta_data_iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('focus_dict.pkl', 'rb') as f:\n",
    "    focus_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,"
     ]
    }
   ],
   "source": [
    "event_id_list = focus_dict['f0']\n",
    "database_path = f'./data/F0/focus_batch_0.db'\n",
    "engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                database_path=database_path,\n",
    "                input_data_folder=input_data_folder,\n",
    "                batch_size=200000,\n",
    "                batch_ids=list(range(1,661,1)),\n",
    "                event_ids=event_id_list,\n",
    "                engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f831811ab5408f3cdb83b41500deeb387c9454d72d6267bf8a6ce625eb23eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
